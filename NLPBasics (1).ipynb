{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AypdyL92orpF",
        "outputId": "f4952d28-828f-46d3-9dfc-1e5095a306e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk scikit-learn\n"
      ]
    },
    {
      "source": [
        "# Re-download the punkt resource\n",
        "import nltk\n",
        "\n",
        "# Delete existing punkt directory if it exists\n",
        "import shutil\n",
        "shutil.rmtree('/root/nltk_data/tokenizers/punkt', ignore_errors=True)\n",
        "\n",
        "# download\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Download the punkt_tab resource"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6rW7JE-q712",
        "outputId": "a5a071d9-77f3-43d8-e3b2-1ae4a05962d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "text = \"Hello World! This is a test sentence to demonstrate basic text preprocessing in NLP.\"\n",
        "tokens = nltk.word_tokenize(text)  # Tokenize text\n",
        "print(tokens)\n",
        "\n",
        "# Sample text\n",
        "text = \"Hello World! This is a test sentence to demonstrate basic text preprocessing in NLP.\"\n",
        "\n",
        "# Step 1: Tokenization\n",
        "tokens = nltk.word_tokenize(text)  # Split text into words\n",
        "from collections import Counter\n",
        "tokens = nltk.word_tokenize(text)  # Tokenize text\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Calculate word frequencies\n",
        "word_frequencies = Counter(tokens)\n",
        "\n",
        "# Print word frequencies\n",
        "print(\"\\nWord Frequencies:\")\n",
        "for word, freq in word_frequencies.items():\n",
        "    print(f\"'{word}': {freq}\")\n",
        "\n",
        "# Step 2: Lowercasing\n",
        "tokens = [word.lower() for word in tokens]  # Convert to lowercase\n",
        "\n",
        "# Step 3: Remove punctuation\n",
        "tokens = [word for word in tokens if word.isalpha()]  # Keep only alphabetic words\n",
        "\n",
        "# Step 4: Remove stopwords\n",
        "nltk.download('stopwords')  # Download stopwords if not already downloaded\n",
        "stop_words = set(stopwords.words('english'))  # Load English stopwords\n",
        "tokens = [word for word in tokens if word not in stop_words]  # Filter out stopwords\n",
        "\n",
        "# Step 5: Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "print(\"Stemmed Tokens:\", stemmed_tokens)\n",
        "\n",
        "# Step 6: Lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')  # Required for lemmatization\n",
        "nltk.download('omw-1.4')  # WordNet morphological data\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n",
        "\n",
        "# Output the final tokens\n",
        "print(\"Preprocessed Tokens:\", tokens)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgKgZ6UTq8T3",
        "outputId": "2d576a87-ce0e-4ce3-b687-1a6923da62ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'World', '!', 'This', 'is', 'a', 'test', 'sentence', 'to', 'demonstrate', 'basic', 'text', 'preprocessing', 'in', 'NLP', '.']\n",
            "Tokens: ['Hello', 'World', '!', 'This', 'is', 'a', 'test', 'sentence', 'to', 'demonstrate', 'basic', 'text', 'preprocessing', 'in', 'NLP', '.']\n",
            "\n",
            "Word Frequencies:\n",
            "'Hello': 1\n",
            "'World': 1\n",
            "'!': 1\n",
            "'This': 1\n",
            "'is': 1\n",
            "'a': 1\n",
            "'test': 1\n",
            "'sentence': 1\n",
            "'to': 1\n",
            "'demonstrate': 1\n",
            "'basic': 1\n",
            "'text': 1\n",
            "'preprocessing': 1\n",
            "'in': 1\n",
            "'NLP': 1\n",
            "'.': 1\n",
            "Stemmed Tokens: ['hello', 'world', 'test', 'sentenc', 'demonstr', 'basic', 'text', 'preprocess', 'nlp']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Tokens: ['hello', 'world', 'test', 'sentence', 'demonstrate', 'basic', 'text', 'preprocessing', 'nlp']\n",
            "Preprocessed Tokens: ['hello', 'world', 'test', 'sentence', 'demonstrate', 'basic', 'text', 'preprocessing', 'nlp']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample sentences for text processing\n",
        "sentences = [\n",
        "    \"Natural Language Processing allows machines to understand and generate human language.\",\n",
        "    \"Language processing helps analyze text and speech data.\",\n",
        "    \"AI and NLP are revolutionizing how humans interact with technology.\",\n",
        "    \"Construction workers build homes, bridges, and skyscrapers that shape cities.\",\n",
        "    \"Safety gear like helmets and gloves is essential for construction workers.\",\n",
        "    \"Heavy machinery like cranes and bulldozers is used at construction sites.\",\n",
        "    \"Construction workers often work in challenging weather conditions.\",\n",
        "    \"Teamwork is crucial for completing large construction projects on time.\",\n",
        "    \"Proper training ensures construction workers can operate equipment safely.\",\n",
        "    \"Construction sites require strict adherence to safety regulations.\",\n",
        "    \"Construction work is both physically demanding and highly rewarding.\"\n",
        "]\n",
        "\n",
        "# Create BoW model\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "print(\"Vocabulary:\", vectorizer.vocabulary_)\n",
        "print(\"BoW Matrix:\\n\", bow_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcAa8yHasJS_",
        "outputId": "ca69f5bf-0cde-42a2-dcf8-9beeefe82bd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'natural': 44, 'language': 39, 'processing': 50, 'allows': 2, 'machines': 43, 'to': 69, 'understand': 71, 'and': 4, 'generate': 26, 'human': 34, 'helps': 30, 'analyze': 3, 'text': 66, 'speech': 62, 'data': 19, 'ai': 1, 'nlp': 45, 'are': 5, 'revolutionizing': 55, 'how': 33, 'humans': 35, 'interact': 37, 'with': 74, 'technology': 65, 'construction': 16, 'workers': 76, 'build': 9, 'homes': 32, 'bridges': 8, 'skyscrapers': 61, 'that': 67, 'shape': 59, 'cities': 13, 'safety': 58, 'gear': 25, 'like': 41, 'helmets': 29, 'gloves': 27, 'is': 38, 'essential': 23, 'for': 24, 'heavy': 28, 'machinery': 42, 'cranes': 17, 'bulldozers': 10, 'used': 72, 'at': 6, 'sites': 60, 'often': 46, 'work': 75, 'in': 36, 'challenging': 12, 'weather': 73, 'conditions': 15, 'teamwork': 64, 'crucial': 18, 'completing': 14, 'large': 40, 'projects': 51, 'on': 47, 'time': 68, 'proper': 52, 'training': 70, 'ensures': 21, 'can': 11, 'operate': 48, 'equipment': 22, 'safely': 57, 'require': 54, 'strict': 63, 'adherence': 0, 'regulations': 53, 'both': 7, 'physically': 49, 'demanding': 20, 'highly': 31, 'rewarding': 56}\n",
            "BoW Matrix:\n",
            " [[0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0\n",
            "  0 0 0 2 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1\n",
            "  0 0 0 0 0]\n",
            " [0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
            "  0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0\n",
            "  0 0 0 0 0]\n",
            " [0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1\n",
            "  0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            "  0 0 1 0 0]\n",
            " [0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0\n",
            "  0 0 0 0 1]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 0\n",
            "  0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 1]\n",
            " [0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            "  0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
            "  1 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 1 0 1 1]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0\n",
            "  0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
            "  0 0 0 0 1]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0\n",
            "  0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            "  0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Use the same sentences\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n",
        "\n",
        "print(\"Vocabulary:\", tfidf_vectorizer.vocabulary_)\n",
        "print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cevF9WhesTVm",
        "outputId": "06f9dd77-f140-440d-f1cf-0435d5afe495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: {'natural': 44, 'language': 39, 'processing': 50, 'allows': 2, 'machines': 43, 'to': 69, 'understand': 71, 'and': 4, 'generate': 26, 'human': 34, 'helps': 30, 'analyze': 3, 'text': 66, 'speech': 62, 'data': 19, 'ai': 1, 'nlp': 45, 'are': 5, 'revolutionizing': 55, 'how': 33, 'humans': 35, 'interact': 37, 'with': 74, 'technology': 65, 'construction': 16, 'workers': 76, 'build': 9, 'homes': 32, 'bridges': 8, 'skyscrapers': 61, 'that': 67, 'shape': 59, 'cities': 13, 'safety': 58, 'gear': 25, 'like': 41, 'helmets': 29, 'gloves': 27, 'is': 38, 'essential': 23, 'for': 24, 'heavy': 28, 'machinery': 42, 'cranes': 17, 'bulldozers': 10, 'used': 72, 'at': 6, 'sites': 60, 'often': 46, 'work': 75, 'in': 36, 'challenging': 12, 'weather': 73, 'conditions': 15, 'teamwork': 64, 'crucial': 18, 'completing': 14, 'large': 40, 'projects': 51, 'on': 47, 'time': 68, 'proper': 52, 'training': 70, 'ensures': 21, 'can': 11, 'operate': 48, 'equipment': 22, 'safely': 57, 'require': 54, 'strict': 63, 'adherence': 0, 'regulations': 53, 'both': 7, 'physically': 49, 'demanding': 20, 'highly': 31, 'rewarding': 56}\n",
            "TF-IDF Matrix:\n",
            " [[0.         0.         0.30661045 0.         0.15435796 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.30661045 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.30661045 0.\n",
            "  0.         0.         0.         0.5241589  0.         0.\n",
            "  0.         0.30661045 0.30661045 0.         0.         0.\n",
            "  0.         0.         0.26207945 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.26207945 0.         0.30661045\n",
            "  0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.38591096 0.19428049 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.38591096 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.38591096 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.32986264 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.32986264 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.38591096 0.         0.         0.\n",
            "  0.38591096 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.        ]\n",
            " [0.         0.32873676 0.         0.         0.16549708 0.32873676\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.32873676 0.         0.32873676\n",
            "  0.         0.32873676 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.32873676 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.32873676 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.32873676\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.32873676 0.         0.        ]\n",
            " [0.         0.         0.         0.         0.17891565 0.\n",
            "  0.         0.         0.35539086 0.35539086 0.         0.\n",
            "  0.         0.35539086 0.         0.         0.16392187 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.35539086 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.35539086\n",
            "  0.         0.35539086 0.         0.         0.         0.\n",
            "  0.         0.35539086 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.23874709]\n",
            " [0.         0.         0.         0.         0.1830891  0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.16774558 0.\n",
            "  0.         0.         0.         0.         0.         0.36368085\n",
            "  0.31086115 0.36368085 0.         0.36368085 0.         0.36368085\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.2443162  0.         0.         0.31086115\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.31086115 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.2443162 ]\n",
            " [0.         0.         0.         0.         0.17392137 0.\n",
            "  0.34547043 0.         0.         0.         0.34547043 0.\n",
            "  0.         0.         0.         0.         0.15934613 0.34547043\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.34547043 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.23208267 0.         0.         0.29529554\n",
            "  0.34547043 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.29529554 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.34547043 0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.39544956 0.         0.         0.39544956 0.18239871 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.39544956 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.39544956 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.39544956 0.         0.33801589 0.26565802]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.3451424  0.         0.15919483 0.\n",
            "  0.3451424  0.         0.         0.         0.         0.\n",
            "  0.29501516 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.2318623  0.         0.3451424  0.\n",
            "  0.         0.         0.         0.         0.         0.3451424\n",
            "  0.         0.         0.         0.3451424  0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.3451424  0.\n",
            "  0.         0.         0.3451424  0.         0.         0.\n",
            "  0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.36121934\n",
            "  0.         0.         0.         0.         0.16661022 0.\n",
            "  0.         0.         0.         0.36121934 0.36121934 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.36121934 0.         0.         0.         0.36121934 0.\n",
            "  0.         0.         0.         0.36121934 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.36121934 0.\n",
            "  0.         0.         0.         0.         0.24266259]\n",
            " [0.39514247 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.18225706 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.39514247\n",
            "  0.39514247 0.         0.         0.         0.3377534  0.\n",
            "  0.3377534  0.         0.         0.39514247 0.         0.\n",
            "  0.         0.         0.         0.3377534  0.         0.\n",
            "  0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.19525086 0.\n",
            "  0.         0.38783848 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.17888814 0.\n",
            "  0.         0.         0.38783848 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.38783848 0.         0.         0.         0.\n",
            "  0.         0.         0.26054499 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.38783848 0.         0.         0.         0.\n",
            "  0.         0.         0.38783848 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.33151021 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Practice with a larger paragraph\n",
        "paragraph = \"\"\"\n",
        "Artificial intelligence (AI) is transforming the way we interact with technology, from voice assistants like Siri and Alexa to sophisticated machine learning models\n",
        "that predict diseases and drive cars autonomously. As AI evolves, it enables new applications across industries, fostering innovation and improving efficiency.\n",
        "\"\"\"\n",
        "\n",
        "# Apply all preprocessing steps\n",
        "tokens = nltk.word_tokenize(paragraph)\n",
        "tokens = [word.lower() for word in tokens]\n",
        "tokens = [word for word in tokens if word.isalpha()]\n",
        "tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "print(\"\\nOriginal Tokens:\", tokens)\n",
        "print(\"Stemmed Tokens:\", stemmed_tokens)\n",
        "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n",
        "\n",
        "# BoW and TF-IDF for the paragraph\n",
        "bow_matrix = vectorizer.fit_transform([\" \".join(tokens)])\n",
        "print(\"BoW Matrix for Paragraph:\\n\", bow_matrix.toarray())\n",
        "\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform([\" \".join(tokens)])\n",
        "print(\"TF-IDF Matrix for Paragraph:\\n\", tfidf_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q82EIpVmO9v6",
        "outputId": "5e4c53f7-b471-411f-c0f9-e6d192b898cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original Tokens: ['artificial', 'intelligence', 'ai', 'transforming', 'way', 'interact', 'technology', 'voice', 'assistants', 'like', 'siri', 'alexa', 'sophisticated', 'machine', 'learning', 'models', 'predict', 'diseases', 'drive', 'cars', 'autonomously', 'ai', 'evolves', 'enables', 'new', 'applications', 'across', 'industries', 'fostering', 'innovation', 'improving', 'efficiency']\n",
            "Stemmed Tokens: ['artifici', 'intellig', 'ai', 'transform', 'way', 'interact', 'technolog', 'voic', 'assist', 'like', 'siri', 'alexa', 'sophist', 'machin', 'learn', 'model', 'predict', 'diseas', 'drive', 'car', 'autonom', 'ai', 'evolv', 'enabl', 'new', 'applic', 'across', 'industri', 'foster', 'innov', 'improv', 'effici']\n",
            "Lemmatized Tokens: ['artificial', 'intelligence', 'ai', 'transforming', 'way', 'interact', 'technology', 'voice', 'assistant', 'like', 'siri', 'alexa', 'sophisticated', 'machine', 'learning', 'model', 'predict', 'disease', 'drive', 'car', 'autonomously', 'ai', 'evolves', 'enables', 'new', 'application', 'across', 'industry', 'fostering', 'innovation', 'improving', 'efficiency']\n",
            "BoW Matrix for Paragraph:\n",
            " [[1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
            "TF-IDF Matrix for Paragraph:\n",
            " [[0.17149859 0.34299717 0.17149859 0.17149859 0.17149859 0.17149859\n",
            "  0.17149859 0.17149859 0.17149859 0.17149859 0.17149859 0.17149859\n",
            "  0.17149859 0.17149859 0.17149859 0.17149859 0.17149859 0.17149859\n",
            "  0.17149859 0.17149859 0.17149859 0.17149859 0.17149859 0.17149859\n",
            "  0.17149859 0.17149859 0.17149859 0.17149859 0.17149859 0.17149859\n",
            "  0.17149859]]\n"
          ]
        }
      ]
    }
  ]
}